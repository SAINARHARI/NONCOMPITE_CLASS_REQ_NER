{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbjMCq1WWHZb",
        "outputId": "4ad64902-d299-47a6-f3f9-f98567c3c338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting pdf_annotate\n",
            "  Downloading pdf_annotate-0.12.0-py3-none-any.whl.metadata (721 bytes)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from pdf_annotate) (24.3.0)\n",
            "Collecting pdfrw>=0.4 (from pdf_annotate)\n",
            "  Downloading pdfrw-0.4-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from pdf_annotate) (11.1.0)\n",
            "Requirement already satisfied: fonttools>=3.44.0 in /usr/local/lib/python3.11/dist-packages (from pdf_annotate) (4.55.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading pdf_annotate-0.12.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfrw-0.4-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfrw, pymupdf, pdf_annotate\n",
            "Successfully installed pdf_annotate-0.12.0 pdfrw-0.4 pymupdf-1.25.2\n"
          ]
        }
      ],
      "source": [
        "pip install transformers pdf_annotate pymupdf\n"
      ]
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wjdXEen_txq",
        "outputId": "0ebf9237-cb56-40a5-822c-dab1903af399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nb-X5xkhXIO",
        "outputId": "3eef4aed-7e4d-42a8-dd6b-623043efffb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Clauses:\n",
            "[MONTHS] months following the voluntary of involuntary termination of Employee’s \n",
            "employment], not to: [INSERT THOSE THAT APPLY] \n",
            "a. [Provide goods or services which directly or indirectly compete with Company]; \n",
            "b. [Invest either directly or indirectly in a business that directly or indirectly competes with \n",
            "\n",
            "Company];  \n",
            "c. [Solicit Company employees to leave their employment]; \n",
            "d. [Engage in any other activities that result in injury to Company]; \n",
            "e. [Other].  \n",
            "\n",
            "\n",
            "Cleaned Summaries:\n",
            "Months months after involuntary termination of employee’s employment, not to: insert those that apply. provide goods or services which directly or indirectly compete with company. .\n",
            "Solicit company employees to leave their employment. engage in any other activities that result in injury to company. other. .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF for text extraction and clause location\n",
        "from pdf_annotate import PdfAnnotator, Location, Appearance\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load NER and summarization models\n",
        "def load_models():\n",
        "    ner_model_name = \"/content/drive/MyDrive/final_model_folder\"  # Path to your trained LegalBERT model\n",
        "    summarizer_model_name = \"/content/drive/MyDrive/t5_summarizer_finetuned-20250121T101029Z-001/t5_summarizer_finetuned\"  # Path to your T5 model\n",
        "\n",
        "    # Load NER model\n",
        "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "    ner_model = AutoModelForSequenceClassification.from_pretrained(ner_model_name)\n",
        "\n",
        "    # Load summarization model\n",
        "    summarizer_tokenizer = T5Tokenizer.from_pretrained(summarizer_model_name)\n",
        "    summarizer_model = T5ForConditionalGeneration.from_pretrained(summarizer_model_name)\n",
        "\n",
        "    ner_pipeline = pipeline(\"text-classification\", model=ner_model, tokenizer=ner_tokenizer)\n",
        "\n",
        "    return ner_pipeline, summarizer_model, summarizer_tokenizer\n",
        "\n",
        "# Extract text and coordinates from PDF\n",
        "def extract_pdf_text_with_coords(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text_data = []\n",
        "    for page_num in range(len(document)):\n",
        "        page = document[page_num]\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "        for block in blocks:\n",
        "            text_data.append({\"text\": block[4], \"bbox\": block[:4], \"page\": page_num})\n",
        "    document.close()\n",
        "    return text_data\n",
        "\n",
        "# Highlight clauses in the PDF\n",
        "def highlight_pdf(input_pdf, output_pdf, highlighted_clauses):\n",
        "    annotator = PdfAnnotator(input_pdf)\n",
        "    for clause, bbox, page_num in highlighted_clauses:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        annotator.add_annotation(\n",
        "            \"square\",\n",
        "            Location(x1=x1, y1=y1, x2=x2, y2=y2, page=page_num),\n",
        "            Appearance(stroke_color=(1, 1, 0), stroke_width=2),\n",
        "        )\n",
        "    annotator.write(output_pdf)\n",
        "\n",
        "# Pre-process clauses (enhanced for context retention)\n",
        "def preprocess_clause(clause):\n",
        "    # Remove extra spaces and preserve context of bracketed sections\n",
        "    clause = re.sub(r'\\[.*?\\]', lambda match: match.group(0).strip('[]'), clause)\n",
        "    clause = re.sub(r'\\s+', ' ', clause.strip())  # Remove extra spaces\n",
        "    return clause\n",
        "\n",
        "# Summarize clauses using the fine-tuned T5 model with anti-repeat settings\n",
        "def summarize_clauses(clauses, model, tokenizer):\n",
        "    summaries = []\n",
        "    for clause in clauses:\n",
        "        processed_clause = preprocess_clause(clause)\n",
        "        inputs = tokenizer.encode(\"summarize: \" + processed_clause, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # Generate the summary with enhanced parameters to reduce repetition\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=70,\n",
        "            min_length=20,\n",
        "            length_penalty=2.0,\n",
        "            repetition_penalty=5.0,  # Increased to further penalize repetition\n",
        "            no_repeat_ngram_size=3,  # Avoid repeating n-grams of size 3 or more\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "def postprocess_summary(summary):\n",
        "    # Remove list markers and extra semicolons, add punctuation\n",
        "    summary = re.sub(r'\\s?[a-e]\\.\\s?', '', summary)\n",
        "    summary = re.sub(r'[;]+', '.', summary)\n",
        "    summary = re.sub(r'\\b(u\\.s\\.|france|other countries)\\b', '', summary, flags=re.IGNORECASE)\n",
        "    summary = re.sub(r'employee\\'s', 'employee\\'s', summary)  # Preserve apostrophes\n",
        "\n",
        "    # Correct specific misspellings or formatting issues\n",
        "    summary = re.sub(r'noto:', 'not to:', summary)\n",
        "    summary = re.sub(r'inserthose', 'insert those', summary)\n",
        "\n",
        "    # Remove extra dots and ensure proper spacing\n",
        "    summary = re.sub(r'\\.\\.+', '.', summary)  # Replace multiple dots with one\n",
        "    summary = re.sub(r'\\s+', ' ', summary.strip())  # Clean extra spaces\n",
        "\n",
        "    # Handle specific cases like ensuring spaces after commas and periods\n",
        "    summary = re.sub(r'([.,])(?!\\s)', r'\\1 ', summary)\n",
        "\n",
        "    # Capitalize the first letter and ensure sentence ends with a period\n",
        "    summary = summary.capitalize()\n",
        "\n",
        "    # Ensure lowercase for 'other' if it's at the end\n",
        "    if summary.endswith('Other.'):\n",
        "        summary = summary[:-5] + 'other.'\n",
        "\n",
        "    if not summary.endswith('.'):\n",
        "        summary += '.'\n",
        "\n",
        "    return summary\n",
        "# Validate Non-Compete Clauses\n",
        "def validate_non_compete_clause(clause):\n",
        "    keywords = [\"compete\", \"restrict\", \"solicit\", \"goods or services\", \"employment termination\"]\n",
        "    return any(keyword in clause.lower() for keyword in keywords)\n",
        "\n",
        "# Main pipeline function\n",
        "def process_pdf(input_pdf, output_pdf):\n",
        "    ner_pipeline, summarizer_model, summarizer_tokenizer = load_models()\n",
        "\n",
        "    text_data = extract_pdf_text_with_coords(input_pdf)\n",
        "\n",
        "    extracted_clauses = []\n",
        "    highlighted_clauses = []\n",
        "    for data in text_data:\n",
        "        prediction = ner_pipeline(data[\"text\"])\n",
        "        predicted_label = prediction[0][\"label\"]\n",
        "        confidence = prediction[0][\"score\"]\n",
        "\n",
        "        if predicted_label == \"LABEL_2\" and confidence > 0.85:\n",
        "            if validate_non_compete_clause(data[\"text\"]):\n",
        "                extracted_clauses.append(data[\"text\"])\n",
        "                highlighted_clauses.append((data[\"text\"], data[\"bbox\"], data[\"page\"]))\n",
        "\n",
        "    highlight_pdf(input_pdf, output_pdf, highlighted_clauses)\n",
        "\n",
        "    summaries = summarize_clauses(extracted_clauses, summarizer_model, summarizer_tokenizer)\n",
        "\n",
        "    cleaned_summaries = [postprocess_summary(summary) for summary in summaries]\n",
        "\n",
        "    return extracted_clauses, cleaned_summaries\n",
        "\n",
        "# Example usage\n",
        "input_pdf_path = \"/content/contract3.pdf\"\n",
        "output_pdf_path = \"highlighted_output.pdf\"\n",
        "\n",
        "extracted_clauses, cleaned_summaries = process_pdf(input_pdf_path, output_pdf_path)\n",
        "\n",
        "print(\"Extracted Clauses:\")\n",
        "for clause in extracted_clauses:\n",
        "    print(clause)\n",
        "\n",
        "print(\"\\nCleaned Summaries:\")\n",
        "for summary in cleaned_summaries:\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF for text extraction and clause location\n",
        "from pdf_annotate import PdfAnnotator, Location, Appearance\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load NER and summarization models\n",
        "def load_models():\n",
        "    ner_model_name = \"/content/drive/MyDrive/final_model_folder\"  # Path to your trained LegalBERT model\n",
        "    summarizer_model_name = \"/content/drive/MyDrive/t5_summarizer_finetuned-20250121T101029Z-001/t5_summarizer_finetuned\"  # Path to your T5 model\n",
        "\n",
        "    # Load NER model\n",
        "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "    ner_model = AutoModelForSequenceClassification.from_pretrained(ner_model_name)\n",
        "\n",
        "    # Load summarization model\n",
        "    summarizer_tokenizer = T5Tokenizer.from_pretrained(summarizer_model_name)\n",
        "    summarizer_model = T5ForConditionalGeneration.from_pretrained(summarizer_model_name)\n",
        "\n",
        "    ner_pipeline = pipeline(\"text-classification\", model=ner_model, tokenizer=ner_tokenizer)\n",
        "\n",
        "    return ner_pipeline, summarizer_model, summarizer_tokenizer\n",
        "\n",
        "# Extract text and coordinates from PDF\n",
        "def extract_pdf_text_with_coords(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text_data = []\n",
        "    for page_num in range(len(document)):\n",
        "        page = document[page_num]\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "        for block in blocks:\n",
        "            text_data.append({\"text\": block[4], \"bbox\": block[:4], \"page\": page_num})\n",
        "    document.close()\n",
        "    return text_data\n",
        "\n",
        "# Highlight clauses in the PDF\n",
        "def highlight_pdf(input_pdf, output_pdf, clauses):\n",
        "    annotator = PdfAnnotator(input_pdf)\n",
        "    doc = fitz.open(input_pdf)  # Open the document for searching text\n",
        "\n",
        "    for clause in clauses:  # Loop through the clauses you want to highlight\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text_instances = page.search_for(clause)\n",
        "            for inst in text_instances:\n",
        "                # Add an annotation for each instance found\n",
        "                annotator.add_annotation(\n",
        "                    \"square\",\n",
        "                    Location(x1=inst[0], y1=inst[1], x2=inst[2], y2=inst[3], page=page_num),\n",
        "                    Appearance(stroke_color=(1, 1, 0), stroke_width=2),\n",
        "                )\n",
        "\n",
        "    doc.close()  # Close the document after searching\n",
        "    annotator.write(output_pdf)\n",
        "\n",
        "# Pre-process clauses (enhanced for context retention)\n",
        "def preprocess_clause(clause):\n",
        "    # Remove extra spaces and preserve context of bracketed sections\n",
        "    clause = re.sub(r'\\[.*?\\]', lambda match: match.group(0).strip('[]'), clause)\n",
        "    clause = re.sub(r'\\s+', ' ', clause.strip())  # Remove extra spaces\n",
        "    return clause\n",
        "\n",
        "# Summarize clauses using the fine-tuned T5 model with anti-repeat settings\n",
        "def summarize_clauses(clauses, model, tokenizer):\n",
        "    summaries = []\n",
        "    for clause in clauses:\n",
        "        processed_clause = preprocess_clause(clause)\n",
        "        inputs = tokenizer.encode(\"summarize: \" + processed_clause, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # Generate the summary with enhanced parameters to reduce repetition\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=70,\n",
        "            min_length=20,\n",
        "            length_penalty=2.0,\n",
        "            repetition_penalty=5.0,  # Increased to further penalize repetition\n",
        "            no_repeat_ngram_size=3,  # Avoid repeating n-grams of size 3 or more\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "def postprocess_summary(summary):\n",
        "    # Remove list markers and extra semicolons, add punctuation\n",
        "    summary = re.sub(r'\\s?[a-e]\\.\\s?', '', summary)\n",
        "    summary = re.sub(r'[;]+', '.', summary)\n",
        "    summary = re.sub(r'\\b(u\\.s\\.|france|other countries)\\b', '', summary, flags=re.IGNORECASE)\n",
        "    summary = re.sub(r'employee\\'s', 'employee\\'s', summary)  # Preserve apostrophes\n",
        "\n",
        "    # Correct specific misspellings or formatting issues\n",
        "    summary = re.sub(r'noto:', 'not to:', summary)\n",
        "    summary = re.sub(r'inserthose', 'insert those', summary)\n",
        "\n",
        "    # Remove extra dots and ensure proper spacing\n",
        "    summary = re.sub(r'\\.\\.+', '.', summary)  # Replace multiple dots with one\n",
        "    summary = re.sub(r'\\s+', ' ', summary.strip())  # Clean extra spaces\n",
        "\n",
        "    # Handle specific cases like ensuring spaces after commas and periods\n",
        "    summary = re.sub(r'([.,])(?!\\s)', r'\\1 ', summary)\n",
        "\n",
        "    # Capitalize the first letter and ensure sentence ends with a period\n",
        "    summary = summary.capitalize()\n",
        "\n",
        "    # Ensure lowercase for 'other' if it's at the end\n",
        "    if summary.endswith('Other.'):\n",
        "        summary = summary[:-5] + 'other.'\n",
        "\n",
        "    if not summary.endswith('.'):\n",
        "        summary += '.'\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Validate Non-Compete Clauses\n",
        "def validate_non_compete_clause(clause):\n",
        "    keywords = [\"compete\", \"restrict\", \"solicit\", \"goods or services\", \"employment termination\"]\n",
        "    return any(keyword in clause.lower() for keyword in keywords)\n",
        "\n",
        "# Main pipeline function\n",
        "def process_pdf(input_pdf, output_pdf):\n",
        "    ner_pipeline, summarizer_model, summarizer_tokenizer = load_models()\n",
        "\n",
        "    text_data = extract_pdf_text_with_coords(input_pdf)\n",
        "\n",
        "    extracted_clauses = []\n",
        "    for data in text_data:\n",
        "        prediction = ner_pipeline(data[\"text\"])\n",
        "        predicted_label = prediction[0][\"label\"]\n",
        "        confidence = prediction[0][\"score\"]\n",
        "\n",
        "        if predicted_label == \"LABEL_2\" and confidence > 0.85:\n",
        "            if validate_non_compete_clause(data[\"text\"]):\n",
        "                extracted_clauses.append(data[\"text\"])\n",
        "\n",
        "    highlight_pdf(input_pdf, output_pdf, extracted_clauses)\n",
        "\n",
        "    summaries = summarize_clauses(extracted_clauses, summarizer_model, summarizer_tokenizer)\n",
        "\n",
        "    cleaned_summaries = [postprocess_summary(summary) for summary in summaries]\n",
        "\n",
        "    return extracted_clauses, cleaned_summaries\n",
        "\n",
        "# Example usage\n",
        "input_pdf_path = \"/content/contract3.pdf\"\n",
        "output_pdf_path = \"highlighted_output.pdf\"\n",
        "\n",
        "extracted_clauses, cleaned_summaries = process_pdf(input_pdf_path, output_pdf_path)\n",
        "\n",
        "print(\"Extracted Clauses:\")\n",
        "for clause in extracted_clauses:\n",
        "    print(clause)\n",
        "\n",
        "print(\"\\nCleaned Summaries:\")\n",
        "for summary in cleaned_summaries:\n",
        "    print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BityKkWHk6hI",
        "outputId": "99b3609e-b938-45ee-d222-d4dfd7e6abab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Clauses:\n",
            "[MONTHS] months following the voluntary of involuntary termination of Employee’s \n",
            "employment], not to: [INSERT THOSE THAT APPLY] \n",
            "a. [Provide goods or services which directly or indirectly compete with Company]; \n",
            "b. [Invest either directly or indirectly in a business that directly or indirectly competes with \n",
            "\n",
            "Company];  \n",
            "c. [Solicit Company employees to leave their employment]; \n",
            "d. [Engage in any other activities that result in injury to Company]; \n",
            "e. [Other].  \n",
            "\n",
            "\n",
            "Cleaned Summaries:\n",
            "Months months after involuntary termination of employee’s employment, not to: insert those that apply. provide goods or services which directly or indirectly compete with company. .\n",
            "Solicit company employees to leave their employment. engage in any other activities that result in injury to company. other. .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF for text extraction and clause location\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load NER and summarization models\n",
        "def load_models():\n",
        "    ner_model_name = \"/content/drive/MyDrive/final_model_folder\"  # Path to your trained LegalBERT model\n",
        "    summarizer_model_name = \"/content/drive/MyDrive/t5_summarizer_finetuned-20250121T101029Z-001/t5_summarizer_finetuned\"  # Path to your T5 model\n",
        "\n",
        "    # Load NER model\n",
        "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
        "    ner_model = AutoModelForSequenceClassification.from_pretrained(ner_model_name)\n",
        "\n",
        "    # Load summarization model\n",
        "    summarizer_tokenizer = T5Tokenizer.from_pretrained(summarizer_model_name)\n",
        "    summarizer_model = T5ForConditionalGeneration.from_pretrained(summarizer_model_name)\n",
        "\n",
        "    ner_pipeline = pipeline(\"text-classification\", model=ner_model, tokenizer=ner_tokenizer)\n",
        "\n",
        "    return ner_pipeline, summarizer_model, summarizer_tokenizer\n",
        "\n",
        "# Extract text and coordinates from PDF\n",
        "def extract_pdf_text_with_coords(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text_data = []\n",
        "    for page_num in range(len(document)):\n",
        "        page = document[page_num]\n",
        "        blocks = page.get_text(\"blocks\")\n",
        "        for block in blocks:\n",
        "            text_data.append({\"text\": block[4], \"bbox\": block[:4], \"page\": page_num})\n",
        "    document.close()\n",
        "    return text_data\n",
        "\n",
        "# Highlight clauses in the PDF\n",
        "def highlight_pdf(input_pdf, output_pdf, clauses):\n",
        "    doc = fitz.open(input_pdf)  # Open the input PDF document\n",
        "    for clause in clauses:  # Loop through the clauses to highlight\n",
        "        clause_text = preprocess_clause(clause)  # Preprocess clause for better matching\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text_instances = page.search_for(clause_text)  # Search for the exact text\n",
        "            if not text_instances:  # Fallback for partial matches if exact text is not found\n",
        "                words = clause_text.split()  # Split the clause into words\n",
        "                for i in range(len(words) - 3):  # Use a sliding window of 3 words\n",
        "                    partial_phrase = \" \".join(words[i:i + 3])\n",
        "                    text_instances += page.search_for(partial_phrase)\n",
        "\n",
        "            for inst in text_instances:\n",
        "                # Add highlight annotation for each instance found\n",
        "                highlight = page.add_highlight_annot(inst)\n",
        "                highlight.set_colors({\"stroke\": (1, 1, 0), \"fill\": (1, 1, 0)})  # Yellow color\n",
        "                highlight.update()\n",
        "\n",
        "    doc.save(output_pdf)  # Save the annotated PDF\n",
        "    doc.close()\n",
        "\n",
        "# Pre-process clauses (enhanced for context retention)\n",
        "def preprocess_clause(clause):\n",
        "    # Remove extra spaces and preserve context of bracketed sections\n",
        "    clause = re.sub(r'\\[.*?\\]', lambda match: match.group(0).strip('[]'), clause)\n",
        "    clause = re.sub(r'\\s+', ' ', clause.strip())  # Remove extra spaces\n",
        "    return clause\n",
        "\n",
        "# Summarize clauses using the fine-tuned T5 model with anti-repeat settings\n",
        "def summarize_clauses(clauses, model, tokenizer):\n",
        "    summaries = []\n",
        "    for clause in clauses:\n",
        "        processed_clause = preprocess_clause(clause)\n",
        "        inputs = tokenizer.encode(\"summarize: \" + processed_clause, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # Generate the summary with enhanced parameters to reduce repetition\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=70,\n",
        "            min_length=20,\n",
        "            length_penalty=2.0,\n",
        "            repetition_penalty=5.0,  # Increased to further penalize repetition\n",
        "            no_repeat_ngram_size=3,  # Avoid repeating n-grams of size 3 or more\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "def postprocess_summary(summary):\n",
        "    # Remove list markers and extra semicolons, add punctuation\n",
        "    summary = re.sub(r'\\s?[a-e]\\.\\s?', '', summary)\n",
        "    summary = re.sub(r'[;]+', '.', summary)\n",
        "    summary = re.sub(r'\\b(u\\.s\\.|france|other countries)\\b', '', summary, flags=re.IGNORECASE)\n",
        "    summary = re.sub(r'employee\\'s', 'employee\\'s', summary)  # Preserve apostrophes\n",
        "\n",
        "    # Correct specific misspellings or formatting issues\n",
        "    summary = re.sub(r'noto:', 'not to:', summary)\n",
        "    summary = re.sub(r'inserthose', 'insert those', summary)\n",
        "\n",
        "    # Remove extra dots and ensure proper spacing\n",
        "    summary = re.sub(r'\\.\\.+', '.', summary)  # Replace multiple dots with one\n",
        "    summary = re.sub(r'\\s+', ' ', summary.strip())  # Clean extra spaces\n",
        "\n",
        "    # Handle specific cases like ensuring spaces after commas and periods\n",
        "    summary = re.sub(r'([.,])(?!\\s)', r'\\1 ', summary)\n",
        "\n",
        "    # Capitalize the first letter and ensure sentence ends with a period\n",
        "    summary = summary.capitalize()\n",
        "\n",
        "    # Ensure lowercase for 'other' if it's at the end\n",
        "    if summary.endswith('Other.'):\n",
        "        summary = summary[:-5] + 'other.'\n",
        "\n",
        "    if not summary.endswith('.'):\n",
        "        summary += '.'\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Validate Non-Compete Clauses\n",
        "def validate_non_compete_clause(clause):\n",
        "    keywords = [\"compete\", \"restrict\", \"solicit\", \"goods or services\", \"employment termination\"]\n",
        "    return any(keyword in clause.lower() for keyword in keywords)\n",
        "\n",
        "# Main pipeline function\n",
        "def process_pdf(input_pdf, output_pdf):\n",
        "    ner_pipeline, summarizer_model, summarizer_tokenizer = load_models()\n",
        "\n",
        "    text_data = extract_pdf_text_with_coords(input_pdf)\n",
        "\n",
        "    extracted_clauses = []\n",
        "    for data in text_data:\n",
        "        prediction = ner_pipeline(data[\"text\"])\n",
        "        predicted_label = prediction[0][\"label\"]\n",
        "        confidence = prediction[0][\"score\"]\n",
        "\n",
        "        if predicted_label == \"LABEL_2\" and confidence > 0.85:\n",
        "            if validate_non_compete_clause(data[\"text\"]):\n",
        "                extracted_clauses.append(data[\"text\"])\n",
        "\n",
        "    highlight_pdf(input_pdf, output_pdf, extracted_clauses)\n",
        "\n",
        "    summaries = summarize_clauses(extracted_clauses, summarizer_model, summarizer_tokenizer)\n",
        "\n",
        "    cleaned_summaries = [postprocess_summary(summary) for summary in summaries]\n",
        "\n",
        "    return extracted_clauses, cleaned_summaries\n",
        "\n",
        "# Example usage\n",
        "input_pdf_path = \"/content/This Employment Agreement.pdf\"\n",
        "output_pdf_path = \"highlighted_output.pdf\"\n",
        "\n",
        "extracted_clauses, cleaned_summaries = process_pdf(input_pdf_path, output_pdf_path)\n",
        "\n",
        "print(\"Extracted Clauses:\")\n",
        "for clause in extracted_clauses:\n",
        "    print(clause)\n",
        "\n",
        "print(\"\\nCleaned Summaries:\")\n",
        "for summary in cleaned_summaries:\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZqbvIb_w_YO",
        "outputId": "58877dd4-0855-4e01-ec17-2780e1267568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Warning: fill color ignored for annot type 'Highlight'.\n",
            "Extracted Clauses:\n",
            "any company that competes with the business of the Company in \n",
            "the geographic area of [Geographic Scope]. \n",
            "\n",
            "The Employee agrees not to solicit, directly or indirectly, any \n",
            "customers, clients, or employees of the Company for the \n",
            "purpose of diverting business away from the Company, both \n",
            "during the term of employment and for a period of [Time \n",
            "Period] following the termination of employment. \n",
            "\n",
            "\n",
            "Cleaned Summaries:\n",
            "A company that competes with the business of geographic scope in geographic areit is not affiliated with other companies, such as those listed on this sit.\n",
            "The employee agrees not to solicit any customers, clients or employees of the company for the purpose of diverting business away from the company. .\n"
          ]
        }
      ]
    }
  ]
}